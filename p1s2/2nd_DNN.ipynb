{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNyZv-Ec52ot"
   },
   "source": [
    "# **Import Libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3m3w1Cw49Zkt"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eso6UHE080D4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zByEi95J86RD"
   },
   "source": [
    "### Load pre-shuffled MNIST data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eRM0QWN83PV"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "4a4Be72j8-ZC",
    "outputId": "32a8adc9-65ae-4398-9fe1-8fa9ab96676a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f968f3e2cc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhB1IgrADSRB2IAnCDiTxf2YjLzDBs2ChAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkmprriw9AnZ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2m4YS4E9CRh"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0Mn0vAYD9DvB",
    "outputId": "bd5f7de2-79eb-423c-90be-e179168cee7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG8JiXR39FHC"
   },
   "outputs": [],
   "source": [
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "fYlFRvKS9HMB",
    "outputId": "1a3a020c-15b3-4da1-a281-9b949ac3d0fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "osKqT73Q9JJB",
    "outputId": "a769dc49-2601-42ba-e995-efe265687efa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\", use_bias=False)`\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\", use_bias=False)`\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", use_bias=False)`\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\", use_bias=False)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\", use_bias=False)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(20, 3, 3, activation='relu', use_bias = False)) #24\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "# model.add(Convolution2D(128, 3, 3, activation='relu', use_bias = False)) \n",
    "\n",
    "model.add(Convolution2D(10, 1, 1, activation='relu', use_bias = False)) #24\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #12\n",
    "model.add(Convolution2D(10, 3, 3, activation='relu', use_bias = False)) #10\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias = False)) #8\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(20, 3, 3, activation='relu', use_bias = False)) #6\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(20, 3, 3, activation='relu', use_bias = False)) #4\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(10, 4, 4))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    " \n",
    "# model.add(Convolution2D(10, 3, 3, input_shape=(28,28,1), use_bias=False)) #26\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #24\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# # model.add(Convolution2D(32, 3, 3, use_bias=False)) #22\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Activation(\"relu\"))\n",
    "\n",
    "# # model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Convolution2D(10, 1, 1, use_bias=False)) # 24\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2))) # 12\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #10\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #8\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #6\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #4\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Convolution2D(16, 3, 3, use_bias=False)) #2\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# # model.add(Convolution2D(10, 1, 1, use_bias=False)) #7\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Activation(\"relu\"))\n",
    "\n",
    "# model.add(Convolution2D(10, 2, 2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "id": "TzdAYg1k9K7Z",
    "outputId": "ddc8cdf2-a925-4793-e65f-abddead7320d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_133 (Conv2D)          (None, 26, 26, 10)        100       \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 26, 26, 10)        40        \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 26, 26, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 24, 24, 20)        1800      \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 24, 24, 20)        80        \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 24, 24, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 24, 24, 10)        200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 24, 24, 10)        40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 10, 10, 10)        900       \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 10, 10, 10)        40        \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 10, 10, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 8, 8, 16)          1440      \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_138 (Conv2D)          (None, 6, 6, 20)          2880      \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 6, 6, 20)          80        \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 6, 6, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 4, 4, 20)          3600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 4, 4, 20)          80        \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 4, 4, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 1, 1, 10)          3210      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,554\n",
      "Trainable params: 14,342\n",
      "Non-trainable params: 212\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp6SuGrL9M3h"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "def scheduler(epoch, lr):\n",
    "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4xWoKhPY9Of5",
    "outputId": "f0c3021b-7e2a-49f6-c54b-8ccd3a758657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0252 - acc: 0.9914 - val_loss: 0.0247 - val_acc: 0.9931\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0007581501.\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0232 - acc: 0.9927 - val_loss: 0.0228 - val_acc: 0.9935\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0006105006.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0215 - acc: 0.9930 - val_loss: 0.0216 - val_acc: 0.9935\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0005109862.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0212 - val_acc: 0.9930\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0004393673.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0194 - acc: 0.9939 - val_loss: 0.0212 - val_acc: 0.9932\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0003853565.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.0218 - val_acc: 0.9930\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0003431709.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0173 - acc: 0.9941 - val_loss: 0.0204 - val_acc: 0.9932\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0003093102.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0212 - val_acc: 0.9935\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0002815315.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0219 - val_acc: 0.9927\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0002583312.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0201 - val_acc: 0.9943\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.0002386635.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0154 - acc: 0.9946 - val_loss: 0.0203 - val_acc: 0.9934\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0002217787.\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0188 - val_acc: 0.9937\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.0002071251.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0191 - val_acc: 0.9940\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0001942879.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0191 - val_acc: 0.9943\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0001829491.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0201 - val_acc: 0.9941\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.0001728608.\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0186 - val_acc: 0.9942\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.000163827.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0135 - acc: 0.9957 - val_loss: 0.0194 - val_acc: 0.9933\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.0001556905.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 0.0183 - val_acc: 0.9941\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.0001483239.\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0186 - val_acc: 0.9939\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.000141623.\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0186 - val_acc: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f968c532908>"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=256, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtsH-lLk-eLb"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mkX8JMv79q9r",
    "outputId": "7d6ae485-ea2f-4dd2-bac0-242cf2d2cec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.018621038327012503, 0.9937]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCWoJkwE9suh"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "Ym7iCFBm9uBs",
    "outputId": "e10df5bc-fe01-4b14-c89c-a88ac9a9f16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3252913e-11 2.1726481e-09 3.1968475e-10 1.0685060e-07 2.4278398e-13\n",
      "  1.1971560e-11 7.5953284e-13 9.9999988e-01 5.4172491e-12 3.3731598e-09]\n",
      " [6.1483298e-08 1.3269543e-05 9.9998426e-01 1.4449780e-09 5.6626186e-07\n",
      "  1.9773632e-11 1.7534541e-06 2.8473911e-08 7.3651827e-09 1.0856602e-09]\n",
      " [4.6638853e-09 9.9998987e-01 7.0925568e-09 5.1582749e-10 2.1913449e-06\n",
      "  6.4338018e-08 3.0530860e-08 7.7288278e-06 4.7828742e-08 1.6173951e-08]\n",
      " [9.9996412e-01 5.9432392e-10 9.1091565e-08 2.6122760e-09 8.3721874e-10\n",
      "  8.9257218e-10 3.1762898e-05 2.5391504e-08 1.1719843e-06 2.7283593e-06]\n",
      " [5.0718960e-12 8.7980612e-10 1.3387157e-11 3.3019516e-12 9.9999881e-01\n",
      "  9.8142258e-12 2.7685474e-11 4.9082693e-09 1.1237298e-09 1.2394773e-06]\n",
      " [3.7468122e-09 9.9996233e-01 1.7935012e-08 3.2648734e-10 1.0691177e-06\n",
      "  2.4525290e-08 1.0333744e-08 3.6610967e-05 1.5261712e-08 8.0849141e-09]\n",
      " [1.1091532e-12 2.2707013e-08 2.1697019e-10 2.9070070e-13 9.9999583e-01\n",
      "  6.3300103e-12 2.8213014e-12 4.5983040e-08 2.9411412e-08 4.1806434e-06]\n",
      " [5.0530508e-10 1.3547268e-09 3.7422671e-10 3.8128118e-09 7.3384026e-06\n",
      "  7.7336386e-09 7.9527795e-10 1.9328555e-08 3.7447126e-08 9.9999261e-01]\n",
      " [1.7010168e-05 7.6037656e-09 2.7594391e-08 3.1983611e-09 3.8568269e-09\n",
      "  9.9808842e-01 1.5105470e-03 2.0122728e-10 3.8333918e-04 5.8452855e-07]]\n",
      "[7 2 1 0 4 1 4 9 5]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:9])\n",
    "print(y_test[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT--y98_dr2T"
   },
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "2GY4Upv4dsUR",
    "outputId": "233a141c-e228-4216-bd9f-7c64eb336c24"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-c098591f49c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mvis_img_in_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-c098591f49c8>\u001b[0m in \u001b[0;36mvis_img_in_filter\u001b[0;34m(img, layer_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n\u001b[1;32m     23\u001b[0m                       layer_name = 'conv2d_133'):\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mimg_ascs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv2d_133'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    #x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
    "                      layer_name = 'conv2d_133'):\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    img_ascs = list()\n",
    "    for filter_index in range(layer_output.shape[3]):\n",
    "        # build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([model.input], [loss, grads])\n",
    "\n",
    "        # step size for gradient ascent\n",
    "        step = 5.\n",
    "\n",
    "        img_asc = np.array(img)\n",
    "        # run gradient ascent for 20 steps\n",
    "        for i in range(20):\n",
    "            loss_value, grads_value = iterate([img_asc])\n",
    "            img_asc += grads_value * step\n",
    "\n",
    "        img_asc = img_asc[0]\n",
    "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
    "        \n",
    "    if layer_output.shape[3] >= 35:\n",
    "        plot_x, plot_y = 6, 6\n",
    "    elif layer_output.shape[3] >= 23:\n",
    "        plot_x, plot_y = 4, 6\n",
    "    elif layer_output.shape[3] >= 11:\n",
    "        plot_x, plot_y = 2, 6\n",
    "    else:\n",
    "        plot_x, plot_y = 1, 2\n",
    "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
    "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
    "    ax[0, 0].set_title('Input image')\n",
    "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
    "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
    "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
    "        if x == 0 and y == 0:\n",
    "            continue\n",
    "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
    "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
    "\n",
    "vis_img_in_filter()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Session_2_DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
